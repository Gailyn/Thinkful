{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final NLP Capstone.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uioajqMUhxSE",
        "colab_type": "text"
      },
      "source": [
        "# NLP Capstone "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPBFtKNrh9LU",
        "colab_type": "text"
      },
      "source": [
        "For this project, I decided create a classifier for classifing 10 different text from 10 different authors. The goals of the project are to use different techiniques to find what technique works the best. I test supervised learning methods and then combine those methods with unsupervised learning feature generation methods to further improve the preformance of the supervised learning models. In addition, I test culstering methods and unsupervised leraning methods like lsa and sentence similarity. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIC2T9wfhuhx",
        "colab_type": "code",
        "outputId": "25b563e9-95ae-4f10-fb86-58430dd07d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "443qH4CUho6l",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt8ANHTmFKRX",
        "colab_type": "code",
        "outputId": "e0abb661-a379-4b07-d139-fbe7cb7f65af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Import the data we just downloaded and installed.\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Grab and process the raw data.\n",
        "print(gutenberg.fileids())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgoHdmaiElML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reading in the data \n",
        "burgess=gutenberg.raw('burgess-busterbrown.txt')\n",
        "sense=gutenberg.raw('austen-sense.txt')\n",
        "bible=gutenberg.raw('bible-kjv.txt')\n",
        "blake=gutenberg.raw('blake-poems.txt')\n",
        "whitman=gutenberg.raw('whitman-leaves.txt')\n",
        "milton=gutenberg.raw('milton-paradise.txt')\n",
        "byrant=gutenberg.raw('bryant-stories.txt')\n",
        "caesar=gutenberg.raw('shakespeare-caesar.txt')\n",
        "ball=gutenberg.raw('chesterton-ball.txt')\n",
        "moby=gutenberg.raw('melville-moby_dick.txt')\n",
        "parents=gutenberg.raw('edgeworth-parents.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96yiRrk7IIEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function for standard text cleaning.\n",
        "def text_cleaner(text):\n",
        "    # Visual inspection identifies a form of punctuation spaCy does not\n",
        "    # recognize: the double dash '--''\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[@#+%*:()'-]\", ' ', text)\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "    \n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "  \n",
        "  \n",
        "parents = text_cleaner(parents[500:50000])\n",
        "ball = text_cleaner(ball[500:50000])\n",
        "caesar = text_cleaner(caesar[500:50000])\n",
        "byrant = text_cleaner(byrant[500:50000])\n",
        "milton = text_cleaner(milton[500:60000])\n",
        "blake = text_cleaner(blake[500:50000])\n",
        "bible = text_cleaner(bible[500:50000])\n",
        "sense = text_cleaner(sense[500:50000])\n",
        "burgess = text_cleaner(burgess[500:50000])\n",
        "moby = text_cleaner(moby[500:50000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrfb_k2yAsf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#parsing data\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "parents_doc = nlp(parents)\n",
        "ball_doc = nlp(ball)\n",
        "caesar_doc = nlp(caesar)\n",
        "byrant_doc = nlp(byrant)\n",
        "milton_doc = nlp(milton)\n",
        "blake_doc = nlp(blake)\n",
        "bible_doc = nlp(bible)\n",
        "sense_doc = nlp(sense)\n",
        "burgess_doc = nlp(burgess)\n",
        "moby_doc = nlp(moby)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WuVxKNpN--7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "moby_sents = [[sents, \"moby\"] for sents in moby_doc.sents]\n",
        "burgess_sents = [[sents,\"burgess\"]for sents in burgess_doc.sents]\n",
        "sense_sents = [[sents, 'sense'] for sents in sense_doc.sents]\n",
        "bible_sents = [[sents, 'bible'] for sents in bible_doc.sents]\n",
        "blake_sents = [[sents, 'blake'] for sents in blake_doc.sents]\n",
        "milton_sents = [[sents, 'milton'] for sents in milton_doc.sents]\n",
        "byrant_sents = [[sents, 'byrant'] for sents in byrant_doc.sents]\n",
        "caesar_sents = [[sents, 'caesar'] for sents in caesar_doc.sents]\n",
        "ball_sents = [[sents, 'ball'] for sents in ball_doc.sents]\n",
        "parents_sents = [[sents, 'parents'] for sents in parents_doc.sents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF8WypHyRHCV",
        "colab_type": "code",
        "outputId": "9c8b916d-a6c6-4be2-8e27-c017677ba6d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Balancing Sentences\n",
        "print(len(moby_sents))\n",
        "print(len(burgess_sents))\n",
        "print(len(bible_sents))\n",
        "print(len(blake_sents))\n",
        "print(len(milton_sents))\n",
        "print(len(sense_sents))\n",
        "print(len(milton_sents))\n",
        "print(len(byrant_sents))  \n",
        "print(len(caesar_sents))\n",
        "print(len(ball_sents))\n",
        "print(len(parents_sents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "692\n",
            "761\n",
            "742\n",
            "600\n",
            "457\n",
            "458\n",
            "457\n",
            "796\n",
            "969\n",
            "594\n",
            "434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWnowPGEYDf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# balancing sentences and lowering number of sentences \n",
        "moby_sents = moby_sents[:300]\n",
        "burgess_sents = burgess_sents[:300]\n",
        "sense_sents = sense_sents[:300]\n",
        "bible_sents = bible_sents[:300]\n",
        "blake_sents = blake_sents[:300]\n",
        "milton_sents = milton_sents[:300]\n",
        "byrant_sents = byrant_sents[:300]\n",
        "caesar_sents = caesar_sents[:300]\n",
        "ball_sents = ball_sents[:300]\n",
        "parents_sents = parents_sents[:300]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyl_f4ozt1l8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "moby_sents = moby_sents[:200]\n",
        "burgess_sents = burgess_sents[:200]\n",
        "sense_sents = sense_sents[:200]\n",
        "bible_sents = bible_sents[:200]\n",
        "blake_sents = blake_sents[:200]\n",
        "milton_sents = milton_sents[:200]\n",
        "byrant_sents = byrant_sents[:200]\n",
        "caesar_sents = caesar_sents[:200]\n",
        "ball_sents = ball_sents[:200]\n",
        "parents_sents = parents_sents[:200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olXp8FUgOxEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = pd.DataFrame(moby_sents+burgess_sents+sense_sents+bible_sents+blake_sents+milton_sents+byrant_sents+caesar_sents+ball_sents+parents_sents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K6TL4CJYkEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function to create a list of the 2000 most common words.\n",
        "def bag_of_words(text):\n",
        "    \n",
        "    # Filter out punctuation and stop words.\n",
        "    allwords = [token.lemma_\n",
        "                for token in text\n",
        "                if not token.is_punct\n",
        "                and not token.is_stop]\n",
        "    \n",
        "    # Return the most common words.\n",
        "    return [item[0] for item in Counter(allwords).most_common(1000)]\n",
        "    \n",
        "\n",
        "# Creates a data frame with features for each word in our common word set.\n",
        "# Each value is the count of the times the word appears in each sentence.\n",
        "def bow_features(sentences, common_words):\n",
        "    \n",
        "    # Scaffold the data frame and initialize counts to zero.\n",
        "    df = pd.DataFrame(columns=common_words)\n",
        "    df['text_sentence'] = sentences[0]\n",
        "    df['text_source'] = sentences[1]\n",
        "    df.loc[:, common_words] = 0\n",
        "    \n",
        "    # Process each row, counting the occurrence of words in each sentence.\n",
        "    for i, sentence in enumerate(df['text_sentence']):\n",
        "        \n",
        "        # Convert the sentence to lemmas, then filter out punctuation,\n",
        "        # stop words, and uncommon words.\n",
        "        words = [token.lemma_\n",
        "                 for token in sentence\n",
        "                 if (\n",
        "                     not token.is_punct\n",
        "                     and not token.is_stop\n",
        "                     and token.lemma_ in common_words\n",
        "                 )]\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Populate the row with word counts.\n",
        "        for word in words:\n",
        "            df.loc[i, word] += 1\n",
        "        \n",
        "        # This counter is just to make sure the kernel didn't hang.\n",
        "        if i % 50 == 0:\n",
        "            print(\"Processing row {}\".format(i))\n",
        "            \n",
        "    return df\n",
        "\n",
        "# Set up the bags.\n",
        "mobywords = bag_of_words(moby_doc)\n",
        "parentswords = bag_of_words(parents_doc)\n",
        "burgesswords = bag_of_words(burgess_doc)\n",
        "sensewords = bag_of_words(sense_doc)\n",
        "miltonwords = bag_of_words(milton_doc)\n",
        "blakewords = bag_of_words(blake_doc)\n",
        "byrantwords = bag_of_words(byrant_doc) \n",
        "ballwords = bag_of_words(ball_doc) \n",
        "biblewords = bag_of_words(bible_doc)\n",
        "caesarwords = bag_of_words(caesar_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIK8AIFcet8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine bags to create a set of unique words.\n",
        "common_words = set(mobywords + parentswords + burgesswords + sensewords + miltonwords + blakewords + byrantwords + ballwords + biblewords + caesarwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNmjmvXWhgF_",
        "colab_type": "code",
        "outputId": "a8ac19cd-9f08-43f3-8108-3a754c2ec2d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        }
      },
      "source": [
        "# GROUP INTO PARAGRAPHS\n",
        "# THEN COMBINE INTO A DATA FRAME \n",
        "# Create our data frame with features. This can take a while to run.\n",
        "word_counts = bow_features(sentences, common_words)\n",
        "word_counts.head()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing row 0\n",
            "Processing row 50\n",
            "Processing row 100\n",
            "Processing row 150\n",
            "Processing row 200\n",
            "Processing row 250\n",
            "Processing row 300\n",
            "Processing row 350\n",
            "Processing row 400\n",
            "Processing row 450\n",
            "Processing row 500\n",
            "Processing row 550\n",
            "Processing row 600\n",
            "Processing row 650\n",
            "Processing row 700\n",
            "Processing row 750\n",
            "Processing row 800\n",
            "Processing row 850\n",
            "Processing row 900\n",
            "Processing row 950\n",
            "Processing row 1000\n",
            "Processing row 1050\n",
            "Processing row 1100\n",
            "Processing row 1150\n",
            "Processing row 1200\n",
            "Processing row 1250\n",
            "Processing row 1300\n",
            "Processing row 1350\n",
            "Processing row 1400\n",
            "Processing row 1450\n",
            "Processing row 1500\n",
            "Processing row 1550\n",
            "Processing row 1600\n",
            "Processing row 1650\n",
            "Processing row 1700\n",
            "Processing row 1750\n",
            "Processing row 1800\n",
            "Processing row 1850\n",
            "Processing row 1900\n",
            "Processing row 1950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weigh</th>\n",
              "      <th>Sunflower</th>\n",
              "      <th>thin</th>\n",
              "      <th>moustache</th>\n",
              "      <th>future</th>\n",
              "      <th>clap</th>\n",
              "      <th>raven</th>\n",
              "      <th>painful</th>\n",
              "      <th>Parish</th>\n",
              "      <th>ducat</th>\n",
              "      <th>Milcah</th>\n",
              "      <th>schoolmaster</th>\n",
              "      <th>nameless</th>\n",
              "      <th>straight</th>\n",
              "      <th>dragon</th>\n",
              "      <th>bladder</th>\n",
              "      <th>keen</th>\n",
              "      <th>Jubal</th>\n",
              "      <th>monster</th>\n",
              "      <th>sum</th>\n",
              "      <th>Mr</th>\n",
              "      <th>Mercy</th>\n",
              "      <th>dimple</th>\n",
              "      <th>caution</th>\n",
              "      <th>fruitful</th>\n",
              "      <th>wonderful</th>\n",
              "      <th>APOLOGY</th>\n",
              "      <th>stale</th>\n",
              "      <th>sinner</th>\n",
              "      <th>voiceless</th>\n",
              "      <th>week</th>\n",
              "      <th>employment</th>\n",
              "      <th>house</th>\n",
              "      <th>private</th>\n",
              "      <th>room</th>\n",
              "      <th>vote</th>\n",
              "      <th>courageous</th>\n",
              "      <th>vineyard</th>\n",
              "      <th>fame</th>\n",
              "      <th>Arkite</th>\n",
              "      <th>...</th>\n",
              "      <th>groping</th>\n",
              "      <th>Cinna</th>\n",
              "      <th>article</th>\n",
              "      <th>church</th>\n",
              "      <th>timed</th>\n",
              "      <th>measure</th>\n",
              "      <th>disapprove</th>\n",
              "      <th>bereave</th>\n",
              "      <th>certainly</th>\n",
              "      <th>closet</th>\n",
              "      <th>pluck</th>\n",
              "      <th>forget</th>\n",
              "      <th>soft</th>\n",
              "      <th>Triumph</th>\n",
              "      <th>innate</th>\n",
              "      <th>didst</th>\n",
              "      <th>Cold</th>\n",
              "      <th>dawn</th>\n",
              "      <th>Ham</th>\n",
              "      <th>wounding</th>\n",
              "      <th>Marks</th>\n",
              "      <th>guinea</th>\n",
              "      <th>alteration</th>\n",
              "      <th>Sinite</th>\n",
              "      <th>expect</th>\n",
              "      <th>laugh</th>\n",
              "      <th>plough</th>\n",
              "      <th>kill</th>\n",
              "      <th>ta</th>\n",
              "      <th>blackberry</th>\n",
              "      <th>Abimael</th>\n",
              "      <th>take</th>\n",
              "      <th>Perizzite</th>\n",
              "      <th>faster</th>\n",
              "      <th>Mightiest</th>\n",
              "      <th>bent</th>\n",
              "      <th>bandy</th>\n",
              "      <th>parliament</th>\n",
              "      <th>text_sentence</th>\n",
              "      <th>text_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(h, them, by, what, name, a, whale, fish, is, ...</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(\", HACKLUYT, \", WHALE, .)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(...)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Sw, .)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(and, Dan, .)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5520 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  weigh  ... text_source\n",
              "0     0  ...        moby\n",
              "1     0  ...        moby\n",
              "2     0  ...        moby\n",
              "3     0  ...        moby\n",
              "4     0  ...        moby\n",
              "\n",
              "[5 rows x 5520 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHTWwOvMuEAu",
        "colab_type": "code",
        "outputId": "7404ed81-acdb-4185-975b-9d4ac48c3537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        }
      },
      "source": [
        "word_counts = bow_features(sentences, common_words)\n",
        "word_counts.head()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing row 0\n",
            "Processing row 50\n",
            "Processing row 100\n",
            "Processing row 150\n",
            "Processing row 200\n",
            "Processing row 250\n",
            "Processing row 300\n",
            "Processing row 350\n",
            "Processing row 400\n",
            "Processing row 450\n",
            "Processing row 500\n",
            "Processing row 550\n",
            "Processing row 600\n",
            "Processing row 650\n",
            "Processing row 700\n",
            "Processing row 750\n",
            "Processing row 800\n",
            "Processing row 850\n",
            "Processing row 900\n",
            "Processing row 950\n",
            "Processing row 1000\n",
            "Processing row 1050\n",
            "Processing row 1100\n",
            "Processing row 1150\n",
            "Processing row 1200\n",
            "Processing row 1250\n",
            "Processing row 1300\n",
            "Processing row 1350\n",
            "Processing row 1400\n",
            "Processing row 1450\n",
            "Processing row 1500\n",
            "Processing row 1550\n",
            "Processing row 1600\n",
            "Processing row 1650\n",
            "Processing row 1700\n",
            "Processing row 1750\n",
            "Processing row 1800\n",
            "Processing row 1850\n",
            "Processing row 1900\n",
            "Processing row 1950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>7000L</th>\n",
              "      <th>ANGEL</th>\n",
              "      <th>cellar</th>\n",
              "      <th>fierceness</th>\n",
              "      <th>morter</th>\n",
              "      <th>ragge</th>\n",
              "      <th>sevenfold</th>\n",
              "      <th>buffet</th>\n",
              "      <th>dell</th>\n",
              "      <th>irreconcilable</th>\n",
              "      <th>prominently</th>\n",
              "      <th>fallen</th>\n",
              "      <th>rushy</th>\n",
              "      <th>disinclination</th>\n",
              "      <th>essence</th>\n",
              "      <th>mayest</th>\n",
              "      <th>week</th>\n",
              "      <th>Toad</th>\n",
              "      <th>orient</th>\n",
              "      <th>mansion</th>\n",
              "      <th>chivalry</th>\n",
              "      <th>oven</th>\n",
              "      <th>wolvish</th>\n",
              "      <th>utmost</th>\n",
              "      <th>mane</th>\n",
              "      <th>monstrous</th>\n",
              "      <th>jesting</th>\n",
              "      <th>compliment</th>\n",
              "      <th>spirit</th>\n",
              "      <th>paint</th>\n",
              "      <th>lift</th>\n",
              "      <th>reverie</th>\n",
              "      <th>humiliation</th>\n",
              "      <th>poor</th>\n",
              "      <th>highly</th>\n",
              "      <th>repute</th>\n",
              "      <th>disposed</th>\n",
              "      <th>surf</th>\n",
              "      <th>drinke</th>\n",
              "      <th>SIXTH</th>\n",
              "      <th>...</th>\n",
              "      <th>ugly</th>\n",
              "      <th>properly</th>\n",
              "      <th>kingdom</th>\n",
              "      <th>Hebrew</th>\n",
              "      <th>freeze</th>\n",
              "      <th>tap</th>\n",
              "      <th>displeas</th>\n",
              "      <th>Marks</th>\n",
              "      <th>doubtful</th>\n",
              "      <th>empyreal</th>\n",
              "      <th>1,119</th>\n",
              "      <th>icy</th>\n",
              "      <th>calmly</th>\n",
              "      <th>scolding</th>\n",
              "      <th>Park</th>\n",
              "      <th>begone</th>\n",
              "      <th>perch</th>\n",
              "      <th>hearte</th>\n",
              "      <th>accord</th>\n",
              "      <th>pitch</th>\n",
              "      <th>presse</th>\n",
              "      <th>return</th>\n",
              "      <th>entertaining</th>\n",
              "      <th>mount</th>\n",
              "      <th>FEGEE</th>\n",
              "      <th>Harpoons</th>\n",
              "      <th>Falling</th>\n",
              "      <th>Irad</th>\n",
              "      <th>bee</th>\n",
              "      <th>mildly</th>\n",
              "      <th>fiend</th>\n",
              "      <th>Neats</th>\n",
              "      <th>Frog</th>\n",
              "      <th>leave</th>\n",
              "      <th>fellow</th>\n",
              "      <th>idea</th>\n",
              "      <th>frequently</th>\n",
              "      <th>particular</th>\n",
              "      <th>text_sentence</th>\n",
              "      <th>text_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(h, them, by, what, name, a, whale, fish, is, ...</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(\", HACKLUYT, \", WHALE, .)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(...)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Sw, .)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(and, Dan, .)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5520 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  7000L ANGEL  ...                                      text_sentence text_source\n",
              "0     0     0  ...  (h, them, by, what, name, a, whale, fish, is, ...        moby\n",
              "1     0     0  ...                         (\", HACKLUYT, \", WHALE, .)        moby\n",
              "2     0     0  ...                                              (...)        moby\n",
              "3     0     0  ...                                            (Sw, .)        moby\n",
              "4     0     0  ...                                      (and, Dan, .)        moby\n",
              "\n",
              "[5 rows x 5520 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqQeaHVYjFPQ",
        "colab_type": "text"
      },
      "source": [
        "After this computationally intensive and time consuming process of processeing the data, cleaning the data, forming a bag of words for each text and combing the bags into a dataframe, the dataset is now ready to be used in clustering analysis. In this section I will test k-means method of clutering to see how well clustering techniques are when it comes to classifying the different text. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf_H6e8po-xl",
        "colab_type": "text"
      },
      "source": [
        "## Clustering Analysis \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25c_-lIp-L0M",
        "colab_type": "text"
      },
      "source": [
        "In this "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIfcvXn9pGr4",
        "colab_type": "code",
        "outputId": "5fea9dbe-bcf8-43eb-8762-1ce7db8df648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = word_counts['text_source']\n",
        "X = word_counts.drop(['text_sentence','text_source'], 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    Y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=0)\n",
        "\n",
        "#fitting the model wsith 10 clusters 10 authors\n",
        "kmeans_pred = KMeans(n_clusters=10, random_state=42).fit_predict(X_train)\n",
        "\n",
        "#Print cross tab of data vs prediction\n",
        "print('Comparing K-Means Clusters to Authors:')\n",
        "print(pd.crosstab(y_train, kmeans_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comparing K-Means Clusters to Authors:\n",
            "col_0        0   1   2   3  4  5   6   7   8    9\n",
            "text_source                                      \n",
            "ball         0  26   0   3  0  1   5   7   3  261\n",
            "bible        0  29   1  28  0  0   0   6   0  234\n",
            "blake        0   2  14   1  0  1  21  21   0  249\n",
            "burgess      0  13   0   0  0  0  29  18  30  193\n",
            "byrant       0  41   1   0  0  5  56   5  13  194\n",
            "caesar       0   3   0   0  0  1   0   4   1  285\n",
            "milton       0   1   0   1  0  0   0  31   0  268\n",
            "moby         1   4   1   0  0  1   1  12   1  279\n",
            "parents      0  25   3   0  4  0  17  21   3  219\n",
            "sense        0   9   0   0  0  0   6  33   0  254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFaCleqApMz8",
        "colab_type": "code",
        "outputId": "889b5bde-8042-4415-a7df-27e88014bfb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "#Show authors categorized by cluster\n",
        "p_y = pd.crosstab(kmeans_pred, y_train)\n",
        "p_y.plot(kind='bar', stacked=False, figsize=[20,5])\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJUAAAE9CAYAAACyQ1P6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1hchz71u3dG",
        "colab_type": "text"
      },
      "source": [
        "While some cluster were able to be dominated by specific authors, the k-means method of clustering failed in the task of classifying the correct text for each author. The majory of the text ended up being in a specific cluster and clusters 4 and 5 were almost empty. The results of this method may mean that custering overall may not be the best for this task. In the next section, I will test supervised learning models in classifying the different text relying only on wordcounts through the bag of words method. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyMiPLN5gyMp",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest Model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-UYa7Dl-K3t",
        "colab_type": "code",
        "outputId": "b59a391b-86ab-403f-efd6-c7ff2417227a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "sfrom sklearn import ensemble\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "rfc = ensemble.RandomForestClassifier()\n",
        "\n",
        "y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "param_grid = { \n",
        "    'n_estimators': [15, 200, 500, 750, 1000, 1500],\n",
        "    'max_features': [1,2,4,6,7,8],\n",
        "    'max_depth': [4,5,6,7,8]\n",
        "}\n",
        "\n",
        "\n",
        "grid = GridSearchCV(rfc, param_grid, cv=5, verbose=3, n_jobs=-1)\n",
        "\n",
        "grid.fit(X,y)\n",
        "\n",
        "# Show the best parameter and best score \n",
        "print(grid.best_params_)\n",
        "print( grid.best_score_)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  8.6min\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 19.7min\n",
            "[Parallel(n_jobs=-1)]: Done 508 tasks      | elapsed: 36.0min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed: 56.9min\n",
            "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 65.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'max_depth': 8, 'max_features': 8, 'n_estimators': 500}\n",
            "0.29772656493304267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B56-6prygwau",
        "colab_type": "code",
        "outputId": "86ac1628-6490-46dd-a134-0dd74d9b30c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "rfc = ensemble.RandomForestClassifier(n_estimators=1500, max_features= 8, max_depth=8)\n",
        "Y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "\n",
        "from sklearn import ensemble\n",
        "rfc = ensemble.RandomForestClassifier(n_estimators=200, max_features= 4, max_depth=4)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size=0.2, random_state=100)\n",
        "print('With 20% Holdout: ' + str(rfc.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(rfc.fit(X, Y).score(X, Y)))\n",
        "\n",
        "\n",
        "# Cross validating using 10 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(rfc, X, Y, cv=5))\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Random Forest report :')\n",
        "print(classification_report(y_test, rfc.predict(X_test)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.18351477449455678\n",
            "Testing on Sample: 0.22454064154469014\n",
            "[0.16614907 0.22049689 0.20839813 0.21028037 0.18338558]\n",
            "Random Forest report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ball       0.00      0.00      0.00        52\n",
            "       bible       1.00      0.01      0.03        71\n",
            "       blake       1.00      0.03      0.05        77\n",
            "      byrant       1.00      0.04      0.07        83\n",
            "      caesar       0.79      0.42      0.54        89\n",
            "        emma       0.00      0.00      0.00        59\n",
            "      milton       0.00      0.00      0.00        34\n",
            "        moby       0.17      1.00      0.29       102\n",
            "     parents       0.00      0.00      0.00        37\n",
            "       sense       0.00      0.00      0.00        39\n",
            "\n",
            "    accuracy                           0.23       643\n",
            "   macro avg       0.40      0.15      0.10       643\n",
            "weighted avg       0.50      0.23      0.14       643\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juNb0IvRlFrj",
        "colab_type": "text"
      },
      "source": [
        "The random forest model did not perfrom well and may mean that this is not the \n",
        "\n",
        "\n",
        "best model for this task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnHUbZFAhcL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy8KOi4lhcId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ9gN2C_hCH9",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhwqqiCCg_2z",
        "colab_type": "code",
        "outputId": "f9304ca2-98be-492d-f09c-892100ed8ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Logistic Regression Gridsearch cv \n",
        "# Model 3: Random Forest gridsearchcv   \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "logr = LogisticRegression()\n",
        "\n",
        "# Create regularization penalty space\n",
        "penalty = ['l1', 'l2']\n",
        "\n",
        "# Create regularization hyperparameter space\n",
        "C = (0.0001,0.001, 0.01, 0.1, 1, 10, 100,1000)\n",
        "\n",
        "# Create hyperparameter options\n",
        "parameters = dict(C=C, penalty=penalty)\n",
        "\n",
        "# Use GS-CV to see which alpha level is best.\n",
        "\n",
        "logr_grid = GridSearchCV(logr, parameters, cv=5, verbose=1)\n",
        "\n",
        "#Fit the logistic regression \n",
        "logr_grid.fit(X, y)\n",
        "\n",
        "#return best parameters and best score\n",
        "\n",
        "print(logr_grid.best_params_)\n",
        "print(logr_grid.best_score_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  4.9min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'C': 1, 'penalty': 'l2'}\n",
            "0.6115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8WjLgSNhH9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(C=1,penalty='l1') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
        "\n",
        "Y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size=0.4, random_state=100)\n",
        "print('With 20% Holdout: ' + str(lr.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(lr.fit(X, Y).score(X, Y)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Cross validating using 5 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(lr, X, Y, cv=5))\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Logistic Regression Report :')\n",
        "print(classification_report(y_test, lr.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwq5hRyHmFGv",
        "colab_type": "text"
      },
      "source": [
        "While there are significant gaps between the holdout group, the test sample, and the results of the cross validation, the logistic regression model perfromed very well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EaLO-Y5hITM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWAjRqNphIgV",
        "colab_type": "text"
      },
      "source": [
        "#Gradient Boosted Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHccqIF_hPfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters to test in gridsearch cv \n",
        "\n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "params = {'n_estimators': [50, 100, 150, 200, 300, 500,750],\n",
        "                           \n",
        "          'max_depth': [4,5,6,7,8],\n",
        "         }\n",
        "\n",
        "\n",
        "y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "\n",
        "\n",
        "# Initialize and fit the model.\n",
        "gb = ensemble.GradientBoostingClassifier()\n",
        "\n",
        "# Use the grid\n",
        "gb_grid = GridSearchCV(gb, params, cv=3, verbose=1, n_jobs=1)\n",
        "\n",
        "# Fit the grid\n",
        "gb_grid.fit(X, y)\n",
        "\n",
        "# Return best parameters and best score\n",
        "print(gb_grid.best_params_)\n",
        "print(gb_grid.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZoZRRR_nWGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiJbQ53HhmFa",
        "colab_type": "code",
        "outputId": "007f2093-23a6-41ac-9cba-22f81f8b940c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "gb = ensemble.GradientBoostingClassifier()\n",
        "\n",
        "y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=20)\n",
        "print('With 20% Holdout: ' + str(gb.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(gb.fit(X, y).score(X, y)))\n",
        "\n",
        "# Cross validating using 10 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(gb,X, y, cv=5))\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Gradient Boosted report :')\n",
        "print(classification_report(y_test, gb.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.603112840466926\n",
            "Testing on Sample: 0.8069137340392402\n",
            "[0.43944099 0.54968944 0.57076205 0.49688474 0.5015674 ]\n",
            "Gradient Boosted report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ball       1.00      0.76      0.87       131\n",
            "       bible       0.97      0.99      0.98       140\n",
            "       blake       0.97      0.71      0.82       173\n",
            "      byrant       0.89      0.69      0.78       156\n",
            "      caesar       1.00      0.75      0.86       187\n",
            "        emma       0.99      0.75      0.85       103\n",
            "      milton       1.00      0.90      0.95        58\n",
            "        moby       0.45      0.99      0.62       185\n",
            "     parents       1.00      0.77      0.87        69\n",
            "       sense       0.98      0.76      0.86        83\n",
            "\n",
            "    accuracy                           0.81      1285\n",
            "   macro avg       0.93      0.81      0.84      1285\n",
            "weighted avg       0.90      0.81      0.83      1285\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCCVJ-9ehma_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYfDOBmOhmoK",
        "colab_type": "text"
      },
      "source": [
        "#SVM Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrwtnVX8Qgvp",
        "colab_type": "code",
        "outputId": "7ffc3b57-794c-4f22-e4f4-c85d69370ba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# SVM gridsearch for best parameters \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "svm = SVC()\n",
        "\n",
        "# new parameters for this model\n",
        "svc_params = [{'C': [.000001,.00001,.001,.01,.1,1,10,100], 'gamma': [.0001,.001,.01,.1]}]\n",
        "\n",
        "# setting up the grid\n",
        "svc_grid = GridSearchCV(svm, svc_params, cv=3, verbose=1, n_jobs=-1)\n",
        "\n",
        "#Fit the grid\n",
        "svc_grid.fit(X,y)\n",
        "\n",
        "#return best parameters and best score\n",
        "\n",
        "print(svc_grid.best_params_)\n",
        "print(svc_grid.best_score_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 30.5min\n",
            "[Parallel(n_jobs=-1)]: Done  84 out of  84 | elapsed: 55.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'C': 10, 'gamma': 0.01}\n",
            "0.46340703830582375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9csGhHAhtYw",
        "colab_type": "code",
        "outputId": "70d79669-6c06-4bab-f344-a530c3431173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC(C=10, gamma=.01)\n",
        "\n",
        "\n",
        "y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "# AttributeError: predict_proba is not available when  probability=False -> ERROR FOR SVM AUC \n",
        "\n",
        "\n",
        "svm.fit(X,y)\n",
        "\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=20)\n",
        "print('With 20% Holdout: ' + str(svm.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(svm.fit(X, y).score(X, y)))\n",
        "\n",
        "# Cross validating using 10 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(svm,X,y, cv=5))\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('SVM report :')\n",
        "print(classification_report(y_test, svm.predict(X_test)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.5816485225505443\n",
            "Testing on Sample: 0.777639364683899\n",
            "[0.4363354  0.52950311 0.53965785 0.49844237 0.48746082]\n",
            "SVM report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ball       1.00      0.70      0.82        60\n",
            "       bible       0.99      0.96      0.97        72\n",
            "       blake       0.97      0.73      0.83        92\n",
            "      byrant       0.91      0.75      0.82        83\n",
            "      caesar       0.94      0.84      0.89        88\n",
            "        emma       1.00      0.65      0.79        49\n",
            "      milton       1.00      0.79      0.88        28\n",
            "        moby       0.48      1.00      0.65       100\n",
            "     parents       1.00      0.77      0.87        30\n",
            "       sense       1.00      0.68      0.81        41\n",
            "\n",
            "    accuracy                           0.81       643\n",
            "   macro avg       0.93      0.79      0.83       643\n",
            "weighted avg       0.89      0.81      0.82       643\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvy_xIQzhuH_",
        "colab_type": "text"
      },
      "source": [
        "# KNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJb2gRCjhy6g",
        "colab_type": "code",
        "outputId": "d6b47312-1b1f-4a08-f8bc-863d630c527c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Model 2: KNN gridsearch\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "\n",
        "# Initialize the model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "\n",
        "y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "# Set parameters for KNN\n",
        "# List of values to try \n",
        "knn_params = [{'n_neighbors': [2,3,5,7,10,15,25,100,250,300]}]\n",
        "\n",
        "#GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, \n",
        "    #n_jobs=None, iid=’warn’, refit=True, cv=’warn’, verbose=0, pre_dispatch=‘2*n_jobs’,\n",
        "    #error_score=’raise-deprecating’, return_train_score=’warn’)\n",
        "\n",
        "# Search for the best paramters. \n",
        "knn_grid = GridSearchCV(knn, knn_params, cv=3, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the grid and obtain results\n",
        "knn_grid.fit(X, y)\n",
        "\n",
        "# Return best parameters and best score\n",
        "print(knn_grid.best_params_)\n",
        "print(knn_grid.best_score_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  9.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'n_neighbors': 3}\n",
            "0.43257552164434754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTKjWQmCueL_",
        "colab_type": "code",
        "outputId": "3b290a4d-4fb0-4278-c353-a1ebcf297cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Nearest neighbors model \n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "\n",
        "\n",
        "knn.fit(X,y)\n",
        "\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=20)\n",
        "print('With 20% Holdout: ' + str(knn.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(knn.fit(X, y).score(X, y)))\n",
        "\n",
        "# Cross validating using 10 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(knn, X, y, cv=5))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('KNN report :')\n",
        "print(classification_report(y_test, knn.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.3297045101088647\n",
            "Testing on Sample: 0.7309249454998443\n",
            "[0.39906832 0.45496894 0.5007776  0.4470405  0.42319749]\n",
            "KNN report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ball       0.78      0.67      0.72        60\n",
            "       bible       0.93      0.90      0.92        72\n",
            "       blake       0.82      0.68      0.75        92\n",
            "      byrant       0.75      0.72      0.74        83\n",
            "      caesar       0.80      0.84      0.82        88\n",
            "        emma       0.75      0.67      0.71        49\n",
            "      milton       0.81      0.61      0.69        28\n",
            "        moby       0.55      0.90      0.68       100\n",
            "     parents       0.83      0.50      0.62        30\n",
            "       sense       0.69      0.44      0.54        41\n",
            "\n",
            "    accuracy                           0.74       643\n",
            "   macro avg       0.77      0.69      0.72       643\n",
            "weighted avg       0.76      0.74      0.74       643\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMgTOwLFhzeU",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes Classfier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aehlADHPh1Oj",
        "colab_type": "code",
        "outputId": "380368f7-68eb-45e7-87cb-fa61fe03071f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate our model and Fit our model to the data.\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X, y)\n",
        "\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=20)\n",
        "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(bnb.fit(X, y).score(X, y)))\n",
        "\n",
        "# Cross validating using 10 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(bnb,X , y, cv=5))\n",
        "\n",
        "#Classification report \n",
        "from sklearn.metrics import classification_report\n",
        "print('Native Bayes Classification report :')\n",
        "print(classification_report(y_test, bnb.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.4287937743190661\n",
            "Testing on Sample: 0.659919028340081\n",
            "[0.43012422 0.48602484 0.46656299 0.4376947  0.47178683]\n",
            "Native Bayes Classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ball       1.00      0.32      0.49       131\n",
            "       bible       0.94      0.87      0.90       140\n",
            "       blake       0.84      0.66      0.74       173\n",
            "      byrant       0.60      0.68      0.64       156\n",
            "      caesar       0.74      0.95      0.83       187\n",
            "        emma       1.00      0.44      0.61       103\n",
            "      milton       1.00      0.28      0.43        58\n",
            "        moby       0.40      0.99      0.57       185\n",
            "     parents       1.00      0.28      0.43        69\n",
            "       sense       1.00      0.28      0.43        83\n",
            "\n",
            "    accuracy                           0.66      1285\n",
            "   macro avg       0.85      0.57      0.61      1285\n",
            "weighted avg       0.80      0.66      0.65      1285\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-5LfRbOwrMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbqNvz_bZQyW",
        "colab_type": "text"
      },
      "source": [
        "## Using TIDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkpJK4k6gP8k",
        "colab_type": "code",
        "outputId": "35ec0c96-34e9-45f3-9155-048e3672649e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Create vectorizer model in order to get tf-idf for each sentence\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
        "                             min_df=2, # only use words that appear at least twice\n",
        "                             stop_words='english', \n",
        "                             lowercase=False, #convert everything to lower casefor\n",
        "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
        "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
        "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.Prevents divide-by-zero errors\n",
        "                            )\n",
        "\n",
        "#convert from spacy object to string\n",
        "sentence_list = word_counts['text_sentence'].astype(str)\n",
        "print(type(sentence_list))\n",
        "\n",
        "#vectorizer model\n",
        "text_tfidf = vectorizer.fit_transform(sentence_list)\n",
        "print(type(text_tfidf))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwWWuftd8z-9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41f49UE68-yW",
        "colab_type": "code",
        "outputId": "667c3af0-6282-408e-b121-f13618ae5734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#List of features\n",
        "features = vectorizer.get_feature_names()\n",
        "\n",
        "#Shape\n",
        "n = text_tfidf.shape[0]\n",
        "\n",
        "\n",
        "tfidf_bysent = [{} for _ in range(0,n)]\n",
        "\n",
        "#for each sentence, lists the feature words and their tf-idf scores\n",
        "for i, j in zip(*text_tfidf.nonzero()):\n",
        "    tfidf_bysent[i][features[j]] = text_tfidf[i, j]\n",
        "\n",
        "#Show first dictionary\n",
        "display(tfidf_bysent[3])\n",
        "print(type(tfidf_bysent))\n",
        "\n",
        "#Create dataframe for this feature set\n",
        "tfidf_df = pd.DataFrame(columns=features)\n",
        "tfidf_df['text_sentence'] = word_counts['text_sentence']\n",
        "tfidf_df['text_source'] = word_counts['text_source']\n",
        "tfidf_df.loc[:, features] = 0\n",
        "\n",
        "counter = 0\n",
        "for i in tfidf_bysent:\n",
        "    for k, v in i.items():\n",
        "        tfidf_df.loc[counter, k] = v\n",
        "    counter = counter + 1\n",
        "print('done!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsFAevo5TENO",
        "colab_type": "code",
        "outputId": "a176de39-0fb0-4181-ccbc-f2934fa542fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "tfidf_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>AND</th>\n",
              "      <th>Abel</th>\n",
              "      <th>Above</th>\n",
              "      <th>Abyss</th>\n",
              "      <th>Adah</th>\n",
              "      <th>Adam</th>\n",
              "      <th>After</th>\n",
              "      <th>Age</th>\n",
              "      <th>Ages</th>\n",
              "      <th>Ah</th>\n",
              "      <th>Alice</th>\n",
              "      <th>All</th>\n",
              "      <th>Almighty</th>\n",
              "      <th>Among</th>\n",
              "      <th>An</th>\n",
              "      <th>And</th>\n",
              "      <th>Angel</th>\n",
              "      <th>Angels</th>\n",
              "      <th>Anne</th>\n",
              "      <th>Annie</th>\n",
              "      <th>Ant</th>\n",
              "      <th>Antonio</th>\n",
              "      <th>Antony</th>\n",
              "      <th>Appeared</th>\n",
              "      <th>...</th>\n",
              "      <th>wish</th>\n",
              "      <th>wished</th>\n",
              "      <th>wishes</th>\n",
              "      <th>woe</th>\n",
              "      <th>woman</th>\n",
              "      <th>women</th>\n",
              "      <th>wonder</th>\n",
              "      <th>wont</th>\n",
              "      <th>woods</th>\n",
              "      <th>word</th>\n",
              "      <th>words</th>\n",
              "      <th>work</th>\n",
              "      <th>worke</th>\n",
              "      <th>working</th>\n",
              "      <th>works</th>\n",
              "      <th>world</th>\n",
              "      <th>worm</th>\n",
              "      <th>worn</th>\n",
              "      <th>worse</th>\n",
              "      <th>worst</th>\n",
              "      <th>worth</th>\n",
              "      <th>wouldn</th>\n",
              "      <th>wound</th>\n",
              "      <th>wounded</th>\n",
              "      <th>wrath</th>\n",
              "      <th>write</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wroth</th>\n",
              "      <th>yards</th>\n",
              "      <th>ye</th>\n",
              "      <th>year</th>\n",
              "      <th>yearly</th>\n",
              "      <th>years</th>\n",
              "      <th>yield</th>\n",
              "      <th>yielding</th>\n",
              "      <th>yon</th>\n",
              "      <th>yonder</th>\n",
              "      <th>young</th>\n",
              "      <th>text_sentence</th>\n",
              "      <th>text_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.352776</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(h, them, by, what, name, a, whale, fish, is, ...</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(\", HACKLUYT, \", WHALE, .)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(...)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Sw, .)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(and, Dan, .)</td>\n",
              "      <td>moby</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1968 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  10 11 12  ... young                                      text_sentence text_source\n",
              "0  0  0  0  ...     0  (h, them, by, what, name, a, whale, fish, is, ...        moby\n",
              "1  0  0  0  ...     0                         (\", HACKLUYT, \", WHALE, .)        moby\n",
              "2  0  0  0  ...     0                                              (...)        moby\n",
              "3  0  0  0  ...     0                                            (Sw, .)        moby\n",
              "4  0  0  0  ...     0                                      (and, Dan, .)        moby\n",
              "\n",
              "[5 rows x 1968 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FHjcrxyTDVH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX2e2rSm9kiM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JP7fND8S_BL",
        "colab_type": "text"
      },
      "source": [
        "##Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG3-vJbJnYTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression Gridsearch cv \n",
        "# Model 3: Random Forest gridsearchcv   \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "y = tfidf_df['text_source']\n",
        "X = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
        "\n",
        "logr = LogisticRegression()\n",
        "\n",
        "# Create regularization penalty space\n",
        "penalty = ['l1', 'l2']\n",
        "\n",
        "# Create regularization hyperparameter space\n",
        "C = (0.0001,0.001, 0.01, 0.1, 1, 10, 100,1000)\n",
        "\n",
        "# Create hyperparameter options\n",
        "parameters = dict(C=C, penalty=penalty)\n",
        "\n",
        "# Use GS-CV to see which alpha level is best.\n",
        "\n",
        "logr_grid = GridSearchCV(logr, parameters, cv=5, verbose=1)\n",
        "\n",
        "#Fit the logistic regression \n",
        "logr_grid.fit(X, y)\n",
        "\n",
        "#return best parameters and best score\n",
        "\n",
        "print(logr_grid.best_params_)\n",
        "print(logr_grid.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t21REa3yT2Lv",
        "colab_type": "code",
        "outputId": "904695c7-506a-4f61-de2f-21035cee2271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(C=10,penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
        "\n",
        "Y = tfidf_df['text_source']\n",
        "X = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size=0.4, random_state=100)\n",
        "print('With 20% Holdout: ' + str(lr.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(lr.fit(X, Y).score(X, Y)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Cross validating using 5 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(lr, X, Y, cv=5))\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Logistic Regression Report :')\n",
        "print(classification_report(y_test, lr.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.66\n",
            "Testing on Sample: 0.86\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.585  0.5675 0.665  0.6725 0.61  ]\n",
            "Logistic Regression Report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ball       1.00      0.89      0.94        81\n",
            "       bible       0.99      0.88      0.93        80\n",
            "       blake       0.97      0.90      0.93        68\n",
            "     burgess       0.99      0.83      0.90        89\n",
            "      byrant       0.98      0.68      0.80        77\n",
            "      caesar       0.99      0.87      0.93        79\n",
            "      milton       1.00      0.94      0.97        81\n",
            "        moby       0.44      1.00      0.61        77\n",
            "     parents       0.99      0.86      0.92        88\n",
            "       sense       0.99      0.84      0.91        80\n",
            "\n",
            "    accuracy                           0.87       800\n",
            "   macro avg       0.93      0.87      0.88       800\n",
            "weighted avg       0.93      0.87      0.88       800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGIccoYlTDCw",
        "colab_type": "text"
      },
      "source": [
        "# SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqdwrzbnUbmA",
        "colab_type": "code",
        "outputId": "108e132c-5116-40cc-fb9a-d2f80852bf9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# SVM gridsearch for best parameters \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "y = tfidf_df['text_source']\n",
        "X = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
        "\n",
        "svm = SVC()\n",
        "\n",
        "# new parameters for this model\n",
        "svc_params = [{'C': [.000001,.00001,.001,.01,.1,1,10,100], 'gamma': [.0001,.001,.01,.1]}]\n",
        "\n",
        "# setting up the grid\n",
        "svc_grid = GridSearchCV(svm, svc_params, cv=3, verbose=1, n_jobs=-1)\n",
        "\n",
        "#Fit the grid\n",
        "svc_grid.fit(X,y)\n",
        "\n",
        "#return best parameters and best score\n",
        "\n",
        "print(svc_grid.best_params_)\n",
        "print(svc_grid.best_score_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  4.5min\n",
            "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed:  9.3min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'C': 10, 'gamma': 0.1}\n",
            "0.565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbFzlsOmUncL",
        "colab_type": "code",
        "outputId": "d9bab518-f3d3-4c30-dd8a-f5fe751aeeaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC(C=10, gamma=.01)\n",
        "\n",
        "y = tfidf_df['text_source']\n",
        "X = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
        "\n",
        "# AttributeError: predict_proba is not available when  probability=False -> ERROR FOR SVM AUC \n",
        "\n",
        "\n",
        "svm.fit(X,y)\n",
        "\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=20)\n",
        "print('With 20% Holdout: ' + str(svm.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(svm.fit(X, y).score(X, y)))\n",
        "\n",
        "# Cross validating using 10 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(svm,X,y, cv=5))\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('SVM report :')\n",
        "print(classification_report(y_test, svm.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.29125\n",
            "Testing on Sample: 0.74\n",
            "[0.5    0.5175 0.4625 0.5275 0.5075]\n",
            "SVM report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ball       0.87      0.80      0.83        69\n",
            "       bible       0.91      0.72      0.80        88\n",
            "       blake       0.89      0.71      0.79        82\n",
            "     burgess       1.00      0.66      0.80        89\n",
            "      byrant       0.94      0.43      0.59        67\n",
            "      caesar       0.96      0.86      0.91        81\n",
            "      milton       0.93      0.80      0.86        70\n",
            "        moby       0.33      0.99      0.49        81\n",
            "     parents       1.00      0.76      0.86        84\n",
            "       sense       0.96      0.76      0.85        89\n",
            "\n",
            "    accuracy                           0.75       800\n",
            "   macro avg       0.88      0.75      0.78       800\n",
            "weighted avg       0.88      0.75      0.78       800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ioiAeP5k2_v",
        "colab_type": "code",
        "outputId": "bcc1bd22-a540-4222-fb8d-b6afa789b1cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "gb = ensemble.GradientBoostingClassifier()\n",
        "\n",
        "\n",
        "y = tfidf_df['text_source']\n",
        "X = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=20)\n",
        "print('With 20% Holdout: ' + str(gb.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(gb.fit(X, y).score(X, y)))\n",
        "\n",
        "# Cross validating using 10 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(gb,X, y, cv=5))\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Gradient Boosted report :')\n",
        "print(classification_report(y_test, gb.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.54875\n",
            "Testing on Sample: 0.8565\n",
            "[0.4875 0.47   0.5475 0.59   0.5275]\n",
            "Gradient Boosted report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ball       1.00      0.87      0.93        69\n",
            "       bible       1.00      0.78      0.88        88\n",
            "       blake       0.96      0.84      0.90        82\n",
            "     burgess       1.00      0.80      0.89        89\n",
            "      byrant       0.96      0.67      0.79        67\n",
            "      caesar       0.99      0.93      0.96        81\n",
            "      milton       1.00      0.94      0.97        70\n",
            "        moby       0.42      0.99      0.59        81\n",
            "     parents       1.00      0.87      0.93        84\n",
            "       sense       1.00      0.84      0.91        89\n",
            "\n",
            "    accuracy                           0.85       800\n",
            "   macro avg       0.93      0.85      0.87       800\n",
            "weighted avg       0.93      0.85      0.87       800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oba5WLaMkd2Q",
        "colab_type": "code",
        "outputId": "49795ad1-8f50-46e5-ef57-91a4ab90f646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "y = tfidf_df['text_source']\n",
        "X = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate our model and Fit our model to the data.\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X, y)\n",
        "\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=20)\n",
        "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(bnb.fit(X, y).score(X, y)))\n",
        "\n",
        "# Cross validating using 10 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(bnb,X , y, cv=5))\n",
        "\n",
        "#Classification report \n",
        "from sklearn.metrics import classification_report\n",
        "print('Native Bayes Classification report :')\n",
        "print(classification_report(y_test, bnb.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.51625\n",
            "Testing on Sample: 0.7155\n",
            "[0.4825 0.52   0.5675 0.6    0.535 ]\n",
            "Native Bayes Classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ball       0.93      0.75      0.83        69\n",
            "       bible       0.96      0.57      0.71        88\n",
            "       blake       1.00      0.63      0.78        82\n",
            "     burgess       0.94      0.71      0.81        89\n",
            "      byrant       0.78      0.60      0.68        67\n",
            "      caesar       0.80      0.90      0.85        81\n",
            "      milton       1.00      0.60      0.75        70\n",
            "        moby       0.30      1.00      0.46        81\n",
            "     parents       1.00      0.71      0.83        84\n",
            "       sense       1.00      0.63      0.77        89\n",
            "\n",
            "    accuracy                           0.71       800\n",
            "   macro avg       0.87      0.71      0.75       800\n",
            "weighted avg       0.87      0.71      0.75       800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qWPbK0QndKi",
        "colab_type": "code",
        "outputId": "e59fbaee-b9c7-4239-936b-000a01c8feca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Model 2: KNN gridsearch\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "\n",
        "# Initialize the model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "\n",
        "\n",
        "y = tfidf_df['text_source']\n",
        "X = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
        "\n",
        "# Set parameters for KNN\n",
        "# List of values to try \n",
        "knn_params = [{'n_neighbors': [2,3,5,7,10,15,25,100,250,300]}]\n",
        "\n",
        "#GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, \n",
        "    #n_jobs=None, iid=’warn’, refit=True, cv=’warn’, verbose=0, pre_dispatch=‘2*n_jobs’,\n",
        "    #error_score=’raise-deprecating’, return_train_score=’warn’)\n",
        "\n",
        "# Search for the best paramters. \n",
        "knn_grid = GridSearchCV(knn, knn_params, cv=3, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the grid and obtain results\n",
        "knn_grid.fit(X, y)\n",
        "\n",
        "# Return best parameters and best score\n",
        "print(knn_grid.best_params_)\n",
        "print(knn_grid.best_score_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.5min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'n_neighbors': 5}\n",
            "0.439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndbc75R3n1fN",
        "colab_type": "code",
        "outputId": "0937da39-5935-4b18-cece-13f40a9e2dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Nearest neighbors model \n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "y = tfidf_df['text_source']\n",
        "X = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
        "\n",
        "\n",
        "\n",
        "knn.fit(X,y)\n",
        "\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=20)\n",
        "print('With 20% Holdout: ' + str(knn.fit(X_train, y_train).score(X_test, y_test)))\n",
        "print('Testing on Sample: ' + str(knn.fit(X, y).score(X, y)))\n",
        "\n",
        "# Cross validating using 10 folds  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(knn, X, y, cv=5))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('KNN report :')\n",
        "print(classification_report(y_test, knn.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 20% Holdout: 0.195\n",
            "Testing on Sample: 0.7645\n",
            "[0.4    0.4175 0.5    0.525  0.5225]\n",
            "KNN report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ball       0.85      0.84      0.85        69\n",
            "       bible       0.93      0.78      0.85        88\n",
            "       blake       0.93      0.84      0.88        82\n",
            "     burgess       0.95      0.71      0.81        89\n",
            "      byrant       0.49      0.66      0.56        67\n",
            "      caesar       0.93      0.65      0.77        81\n",
            "      milton       0.97      0.54      0.70        70\n",
            "        moby       0.40      0.96      0.56        81\n",
            "     parents       1.00      0.81      0.89        84\n",
            "       sense       0.99      0.76      0.86        89\n",
            "\n",
            "    accuracy                           0.76       800\n",
            "   macro avg       0.85      0.76      0.77       800\n",
            "weighted avg       0.85      0.76      0.78       800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T-KJ1C-OHq4",
        "colab_type": "text"
      },
      "source": [
        "## LSA and Sentence Similarity "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paHZrasTOGZM",
        "colab_type": "code",
        "outputId": "1aa85076-93c1-4910-beee-9c0f858aee27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "\n",
        "Y = tfidf_df['text_source']\n",
        "X = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to create the necessary training and test groups\n",
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X, Y ,test_size=0.4, random_state=100)\n",
        "\n",
        "\n",
        "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 150.\n",
        "svd= TruncatedSVD(150)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "# Run SVD on the training data, then project the training data.\n",
        "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
        "\n",
        "variance_explained=svd.explained_variance_ratio_\n",
        "total_variance = variance_explained.sum()\n",
        "print(\"Percent variance captured by all components:\",total_variance*100)\n",
        "\n",
        "#Looking at what sorts of paragraphs our solution considers similar, for the first 10 identified topics\n",
        "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train_tfidf)\n",
        "for i in range(10):\n",
        "    print('Component {}:'.format(i))\n",
        "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent variance captured by all components: 45.33705083701683\n",
            "Component 0:\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                                      0.629483\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3858103322676233, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                       0.594763\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3858103322676233, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                       0.538113\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.40940808529994066, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                      0.533792\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1558426402286594, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                       0.500665\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.19807823896217175, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                      0.500125\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14384139957765293, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.26182124782297816, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.494223\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3618699880889219, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                       0.491033\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3987867260038527, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                       0.489667\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3427373940633048, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                       0.463992\n",
            "Name: 0, dtype: float64\n",
            "Component 1:\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                                     0.616902\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3858103322676233, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                      0.599246\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3858103322676233, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                      0.537230\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.40940808529994066, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.532523\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3987867260038527, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                      0.497348\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3618699880889219, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                      0.469110\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.30446398332663044, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.453626\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3427373940633048, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                      0.444756\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.30153149034169074, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.403235\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5047493821263791, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.24282372072759176, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.398901\n",
            "Name: 1, dtype: float64\n",
            "Component 2:\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.979138\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.977621\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.977621\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.977621\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.977621\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.977621\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.964795\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.907500\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.824154\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.811389\n",
            "Name: 2, dtype: float64\n",
            "Component 3:\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.635843\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.635843\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.617396\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.480661\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.434079\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.395573\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.395573\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                     0.395573\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1558426402286594, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.389677\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6611737950847888, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.339493\n",
            "Name: 3, dtype: float64\n",
            "Component 4:\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3982988696568181, 0, 0, 0, 0, 0, 0.3166957244642672, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)      0.568135\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14384139957765293, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.26182124782297816, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)    0.490779\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3639386410068548, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                       0.482087\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                                        0.477041\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                                        0.477041\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                                        0.477041\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                                        0.477041\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5274910836444315, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                       0.476804\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.16072200433602515, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                      0.469541\n",
            "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3816606541651458, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...)                       0.468215\n",
            "Name: 4, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_gdzPK7SBdI",
        "colab_type": "code",
        "outputId": "623f45c1-b531-481d-9765-26a87d55ebfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "# sentence similarity among the 10 authors \n",
        "\n",
        "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
        "\n",
        "# with the test set data.  \n",
        "# Compute document similarity using LSA components\n",
        "similarity = np.asarray(np.asmatrix(X_test_lsa) * np.asmatrix(X_test_lsa).T)\n",
        "#Only taking the first 10 sentences\n",
        "sim_matrix=pd.DataFrame(similarity,index=X_test_tfidf).iloc[0:10,0:10]\n",
        "#Making a plot\n",
        "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
        "plt.show()\n",
        "\n",
        "#Generating a key for the plot.\n",
        "print('Key:')\n",
        "for i in range(10):\n",
        "    print(i,sim_matrix.index[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Key:\n",
            "0 (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5785170381294951, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.613758768902711, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5372319891712567, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "1 (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5105790515547186, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5766081528390646, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4371721808735884, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3326170828870275, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.32415803342653854, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "2 (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.38785191358198473, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.30821959637160523, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.30465305418317, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.38785191358198473, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.38785191358198473, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.38785191358198473, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.26712620694647876, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3729803735787568, 0, 0, 0, 0)\n",
            "3 (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "4 (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.26530792042476437, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.24063115716437633, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.30746098472390715, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.19025659468487538, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25528977203223463, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.29908375954429195, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2599849578096678, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.28761588755856166, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.29908375954429195, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.29908375954429195, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.23235402806077396, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.20002791253444693, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.27145282979539814, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.29908375954429195, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "5 (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.36284576413573577, 0, 0.36284576413573577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.37731321518301447, 0, 0, 0, 0, 0, 0, 0, 0, 0.33470284251312726, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.37731321518301447, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.37731321518301447, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2567237318688389, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.36284576413573577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "6 (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.35431824305126947, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3903839657401073, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3182525203624317, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3903839657401073, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3638047689896943, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.27605832969367505, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3277390463008565, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3903839657401073, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "7 (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.49697567279156174, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.47912021184726455, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4415301264399941, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5731580503654202, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "8 (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7015651322110334, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7126053362596405, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
            "9 (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6029014345850525, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6796556522008504, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4178253877000799, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfzeFr-shd2T",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCa2OopLnY0v",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpC9oBRV9vjP",
        "colab_type": "text"
      },
      "source": [
        "- Clustering was not the best method for this task While some cluster were able to be dominated by specific authors, the k-means method of clustering failed in the task of classifying the correct text for each author. The majory of the text ended up being in a specific cluster and clusters 4 and 5 were almost empty.\n",
        "- The best performing models included Logistic regression, SVM and gradient boosted classifier. However, each of the models deomstrated a large gap between the results of the holdout group and the results of test group. For this kind of data logistric regression most likely performed the best due to the reduction of the large feature space\n",
        "- TF-dif methods performed much better than BoW method and engender more accurate results \n",
        "- the sentence similarity shows there not to be a large difference between the different authors and suggest that to imporve this project, more data or a more intensive cleaning process is needed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amZA23CZo2Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}