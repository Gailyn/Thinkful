{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install spacy\n!python -m spacy download 'en'",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting spacy\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/0fab3fa533229436533fb504bb62f4cf7ea29541a487a9d1a0749876fc23/spacy-2.1.4-cp36-cp36m-manylinux1_x86_64.whl (29.8MB)\n\u001b[K     |████████████████████████████████| 29.8MB 38kB/s  eta 0:00:01     |██████████████████████▏         | 20.7MB 8.1MB/s eta 0:00:02\n\u001b[?25hCollecting cymem<2.1.0,>=2.0.2 (from spacy)\n  Downloading https://files.pythonhosted.org/packages/3d/61/9b0520c28eb199a4b1ca667d96dd625bba003c14c75230195f9691975f85/cymem-2.0.2-cp36-cp36m-manylinux1_x86_64.whl\nCollecting thinc<7.1.0,>=7.0.2 (from spacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/f1/3df317939a07b2fc81be1a92ac10bf836a1d87b4016346b25f8b63dee321/thinc-7.0.4-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n\u001b[K     |████████████████████████████████| 2.1MB 6.3MB/s eta 0:00:01\n\u001b[?25hCollecting srsly<1.1.0,>=0.0.5 (from spacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/97/47753e3393aa4b18de9f942fac26f18879d1ae950243a556888f389d1398/srsly-0.0.5-cp36-cp36m-manylinux1_x86_64.whl (180kB)\n\u001b[K     |████████████████████████████████| 184kB 24.1MB/s eta 0:00:01\n\u001b[?25hCollecting wasabi<1.1.0,>=0.2.0 (from spacy)\n  Downloading https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (2.21.0)\nCollecting plac<1.0.0,>=0.9.6 (from spacy)\n  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\nRequirement already satisfied: numpy>=1.15.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (1.16.2)\nCollecting blis<0.3.0,>=0.2.2 (from spacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n\u001b[K     |████████████████████████████████| 3.2MB 23.9MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (2.6.0)\nCollecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n  Downloading https://files.pythonhosted.org/packages/a6/e6/63f160a4fdf0e875d16b28f972083606d8d54f56cd30cb8929f9a1ee700e/murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl\nCollecting preshed<2.1.0,>=2.0.1 (from spacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n\u001b[K     |████████████████████████████████| 92kB 13.8MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.26.0)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.23)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.10.15)\nRequirement already satisfied: idna<2.9,>=2.5 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\nInstalling collected packages: cymem, srsly, preshed, plac, wasabi, blis, murmurhash, thinc, spacy\nSuccessfully installed blis-0.2.4 cymem-2.0.2 murmurhash-1.0.2 plac-0.9.6 preshed-2.0.1 spacy-2.1.4 srsly-0.0.5 thinc-7.0.4 wasabi-0.2.2\n\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nCollecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n\u001b[K     |████████████████████████████████| 11.1MB 3.8MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-li78nc8q/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\nSuccessfully built en-core-web-sm\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-2.1.0\n\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('en_core_web_sm')\n\u001b[38;5;2m✔ Linking successful\u001b[0m\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/en_core_web_sm -->\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/spacy/data/en\nYou can now load the model via spacy.load('en')\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport sklearn\nimport spacy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom nltk.corpus import gutenberg, stopwords\nfrom collections import Counter\nimport nltk\n\nnltk.download('gutenberg')\n",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package gutenberg to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "cell_type": "markdown",
      "source": "Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n\nOur feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.\n\n**Note**: Since processing all the text takes around ~5-10 minutes, in the cell below we are taking only the first tenth of each text. If you want to experiment, feel free to change the following code in the next cell:\n\n```python\nalice = text_cleaner(alice[:int(len(alice)/10)])\npersuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])\n```\nto \n\n```python\nalice = text_cleaner(alice)\npersuasion = text_cleaner(persuasion)\n```"
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Utility function for standard text cleaning.\ndef text_cleaner(text):\n    # Visual inspection identifies a form of punctuation spaCy does not\n    # recognize: the double dash '--'.  Better get rid of it now!\n    text = re.sub(r'--',' ',text)\n    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n    text = ' '.join(text.split())\n    return text\n    \n# Load and clean the data.\npersuasion = gutenberg.raw('austen-persuasion.txt')\nalice = gutenberg.raw('carroll-alice.txt')\n\n# The Chapter indicator is idiosyncratic\npersuasion = re.sub(r'Chapter \\d+', '', persuasion)\nalice = re.sub(r'CHAPTER .*', '', alice)\n    \nalice = text_cleaner(alice[:int(len(alice)/10)])\npersuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Parse the cleaned novels. This can take a bit.\nnlp = spacy.load('en')\nalice_doc = nlp(alice)\npersuasion_doc = nlp(persuasion)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Group into sentences.\nalice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\npersuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n\n# Combine the sentences from the two novels into one data frame.\nsentences = pd.DataFrame(alice_sents + persuasion_sents)\nsentences.head()",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(So, she, was, considering, in, her, own, mind...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(Oh, dear, !)</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(Oh, dear, !)</td>\n      <td>Carroll</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                                   0        1\n0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n1  (So, she, was, considering, in, her, own, mind...  Carroll\n2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n3                                      (Oh, dear, !)  Carroll\n4                                      (Oh, dear, !)  Carroll"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "cell_type": "markdown",
      "source": "Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Utility function to create a list of the 2000 most common words.\ndef bag_of_words(text):\n    \n    # Filter out punctuation and stop words.\n    allwords = [token.lemma_\n                for token in text\n                if not token.is_punct\n                and not token.is_stop]\n    \n    # Return the most common words.\n    return [item[0] for item in Counter(allwords).most_common(2000)]\n    \n\n# Creates a data frame with features for each word in our common word set.\n# Each value is the count of the times the word appears in each sentence.\ndef bow_features(sentences, common_words):\n    \n    # Scaffold the data frame and initialize counts to zero.\n    df = pd.DataFrame(columns=common_words)\n    df['text_sentence'] = sentences[0]\n    df['text_source'] = sentences[1]\n    df.loc[:, common_words] = 0\n    \n    # Process each row, counting the occurrence of words in each sentence.\n    for i, sentence in enumerate(df['text_sentence']):\n        \n        # Convert the sentence to lemmas, then filter out punctuation,\n        # stop words, and uncommon words.\n        words = [token.lemma_\n                 for token in sentence\n                 if (\n                     not token.is_punct\n                     and not token.is_stop\n                     and token.lemma_ in common_words\n                 )]\n        \n        # Populate the row with word counts.\n        for word in words:\n            df.loc[i, word] += 1\n        \n        # This counter is just to make sure the kernel didn't hang.\n        if i % 50 == 0:\n            print(\"Processing row {}\".format(i))\n            \n    return df\n\n# Set up the bags.\nalicewords = bag_of_words(alice_doc)\npersuasionwords = bag_of_words(persuasion_doc)\n\n# Combine bags to create a set of unique words.\ncommon_words = set(alicewords + persuasionwords)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create our data frame with features. This can take a while to run.\nword_counts = bow_features(sentences, common_words)\nword_counts.head()",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Processing row 0\nProcessing row 50\nProcessing row 100\nProcessing row 150\nProcessing row 200\nProcessing row 250\nProcessing row 300\nProcessing row 350\nProcessing row 400\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>distinction</th>\n      <th>mild</th>\n      <th>come</th>\n      <th>alienable</th>\n      <th>restriction</th>\n      <th>avoid</th>\n      <th>utmost</th>\n      <th>acknowledge</th>\n      <th>confide</th>\n      <th>professed</th>\n      <th>...</th>\n      <th>high</th>\n      <th>short</th>\n      <th>correct</th>\n      <th>oppose</th>\n      <th>pursuit</th>\n      <th>hardly</th>\n      <th>young</th>\n      <th>powder</th>\n      <th>text_sentence</th>\n      <th>text_source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>(So, she, was, considering, in, her, own, mind...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>(Oh, dear, !)</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>(Oh, dear, !)</td>\n      <td>Carroll</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1614 columns</p>\n</div>",
            "text/plain": "  distinction mild come alienable restriction avoid utmost acknowledge  \\\n0           0    0    0         0           0     0      0           0   \n1           0    0    0         0           0     0      0           0   \n2           0    0    0         0           0     0      0           0   \n3           0    0    0         0           0     0      0           0   \n4           0    0    0         0           0     0      0           0   \n\n  confide professed     ...     high short correct oppose pursuit hardly  \\\n0       0         0     ...        0     0       0      0       0      0   \n1       0         0     ...        0     0       0      0       0      0   \n2       0         0     ...        0     0       0      0       0      0   \n3       0         0     ...        0     0       0      0       0      0   \n4       0         0     ...        0     0       0      0       0      0   \n\n  young powder                                      text_sentence text_source  \n0     0      0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n1     0      0  (So, she, was, considering, in, her, own, mind...     Carroll  \n2     0      0  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n3     0      0                                      (Oh, dear, !)     Carroll  \n4     0      0                                      (Oh, dear, !)     Carroll  \n\n[5 rows x 1614 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "cell_type": "markdown",
      "source": "## Trying out BoW\n\nNow let's give the bag of words features a whirl by trying a random forest."
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import ensemble\nfrom sklearn.model_selection import train_test_split\n\nrfc = ensemble.RandomForestClassifier()\nY = word_counts['text_source']\nX = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    Y,\n                                                    test_size=0.4,\n                                                    random_state=0)\ntrain = rfc.fit(X_train, y_train)\n\nprint('Training set score:', rfc.score(X_train, y_train))\nprint('\\nTest set score:', rfc.score(X_test, y_test))",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training set score: 0.9774436090225563\n\nTest set score: 0.8651685393258427\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "cell_type": "markdown",
      "source": "Holy overfitting, Batman! Overfitting is a known problem when using bag of words, since it basically involves throwing a massive number of features at a model – some of those features (in this case, word frequencies) will capture noise in the training set. Since overfitting is also a known problem with Random Forests, the divergence between training score and test score is expected.\n\n\n## BoW with Logistic Regression\n\nLet's try a technique with some protection against overfitting due to extraneous features – logistic regression with ridge regularization (from ridge regression, also called L2 regularization)."
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\ntrain = lr.fit(X_train, y_train)\nprint(X_train.shape, y_train.shape)\nprint('Training set score:', lr.score(X_train, y_train))\nprint('\\nTest set score:', lr.score(X_test, y_test))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(266, 1612) (266,)\nTraining set score: 0.9699248120300752\n\nTest set score: 0.8764044943820225\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(penalty='l1') # No need to specify l2 as it's the default. But we put it for demonstration.\ntrain = lr.fit(X_train, y_train)\nprint(X_train.shape, y_train.shape)\nprint('Training set score:', lr.score(X_train, y_train))\nprint('\\nTest set score:', lr.score(X_test, y_test))",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(266, 1612) (266,)\nTraining set score: 0.8984962406015038\n\nTest set score: 0.8314606741573034\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "cell_type": "markdown",
      "source": "Logistic regression performs a bit better than the random forest.  \n\n# BoW with Gradient Boosting\n\nAnd finally, let's see what gradient boosting can do:"
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "clf = ensemble.GradientBoostingClassifier()\ntrain = clf.fit(X_train, y_train)\n\nprint('Training set score:', clf.score(X_train, y_train))\nprint('\\nTest set score:', clf.score(X_test, y_test))",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training set score: 0.9661654135338346\n\nTest set score: 0.8258426966292135\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "cell_type": "markdown",
      "source": "Looks like logistic regression is the winner, but there's room for improvement.\n\n# Same model, new inputs\n\nWhat if we feed the model a different novel by Jane Austen, like _Emma_?  Will it be able to distinguish Austen from Carroll with the same level of accuracy if we insert a different sample of Austen's writing?\n\nFirst, we need to process _Emma_ the same way we processed the other data, and combine it with the Alice data. Remember that for computation time concerns, we only took the first tenth of the Alice text. Emma is pretty long. **So in order to get comparable length texts, we take the first sixtieth of Emma**. Again, if you want to experiment, you can take the whole texts of each."
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Clean the Emma data.\nemma = gutenberg.raw('austen-emma.txt')\nemma = re.sub(r'VOLUME \\w+', '', emma)\nemma = re.sub(r'CHAPTER \\w+', '', emma)\nemma = text_cleaner(emma[:int(len(emma)/60)])\nprint(emma[:100])",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Parse our cleaned data.\nemma_doc = nlp(emma)",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Group into sentences.\npersuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\nemma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Build a new Bag of Words data frame for Emma word counts.\n# We'll use the same common words from Alice and Persuasion.\nemma_sentences = pd.DataFrame(emma_sents)\nemma_bow = bow_features(emma_sentences, common_words)\n\nprint('done')",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Processing row 0\nProcessing row 50\nProcessing row 100\nProcessing row 150\ndone\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Now we can model it!\n# Let's use logistic regression again.\n\n# Combine the Emma sentence data with the Alice data from the test set.\nX_Emma_test = np.concatenate((\n    X_train[y_train[y_train=='Carroll'].index],\n    emma_bow.drop(['text_sentence','text_source'], 1)\n), axis=0)\ny_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n                         pd.Series(['Austen'] * emma_bow.shape[0])])\n\n# Model.\nprint('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\nlr_Emma_predicted = lr.predict(X_Emma_test)\npd.crosstab(y_Emma_test, lr_Emma_predicted)",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\nTest set score: 0.7073170731707317\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>col_0</th>\n      <th>Austen</th>\n      <th>Carroll</th>\n    </tr>\n    <tr>\n      <th>row_0</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Austen</th>\n      <td>160</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>Carroll</th>\n      <td>62</td>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "col_0    Austen  Carroll\nrow_0                   \nAusten      160       10\nCarroll      62       14"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "cell_type": "markdown",
      "source": "Well look at that!  NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland.  Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\n\n# Challenge 0:\n\nRecall that the logistic regression model's best performance on the test set was 93%.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%.  \n\n# Challenge 1:\nFind out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n\nRecord your work for each challenge in a notebook and submit it below."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 1"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# SVM gridsearch for best parameters \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nsvm = SVC()\n\n# new parameters for this model\nsvc_params = [{'C': [.1,1,10,100,1000], 'gamma': [.0001,.001,.01,.1]}]\n\n# setting up the grid\nsvc_grid = GridSearchCV(svm, svc_params, cv=7, verbose=1, n_jobs=-1)\n\n#Fit the grid\nsvc_grid.fit(X_Emma_test, y_Emma_test)\n\n#return best parameters and best score\n\nprint(svc_grid.best_params_)\nprint(svc_grid.best_score_)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Fitting 7 folds for each of 20 candidates, totalling 140 fits\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    5.5s\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "{'C': 10, 'gamma': 0.01}\n0.7804878048780488\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:   17.3s finished\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# SVM model using pca \nfrom sklearn.svm import SVC\nsvm = SVC(C=10, gamma=.01)\n\nsvm.fit(X_Emma_test, y_Emma_test)\n\n# Use train_test_split to create the necessary training and test groups\nX_train, X_test, y_train, y_test = train_test_split(X_Emma_test, y_Emma_test, test_size=0.4, random_state=2000)\nprint('With 20% Holdout: ' + str(svm.fit(X_train, y_train).score(X_test, y_test)))\nprint('Testing on Sample: ' + str(svm.fit(X_Emma_test, y_Emma_test).score(X_Emma_test, y_Emma_test)))\n\n# Cross validating using 10 folds  \nfrom sklearn.model_selection import cross_val_score\nprint(cross_val_score(svm, X_Emma_test, y_Emma_test, cv=5))\n\nfrom sklearn.metrics import classification_report\nprint('SVM report :')\nprint(classification_report(y_test, svm.predict(X_test)))",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "With 20% Holdout: 0.8484848484848485\nTesting on Sample: 0.9065040650406504\n[0.7        0.73469388 0.81632653 0.83673469 0.79591837]\nSVM report :\n              precision    recall  f1-score   support\n\n      Austen       0.93      1.00      0.96        68\n     Carroll       1.00      0.84      0.91        31\n\n   micro avg       0.95      0.95      0.95        99\n   macro avg       0.97      0.92      0.94        99\nweighted avg       0.95      0.95      0.95        99\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 2\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "(print(gutenberg.fileids()))",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Clean the shakespeare data.\nshake = gutenberg.raw('shakespeare-caesar.txt')\nshake = re.sub(r'ACT \\w+', '', shake)\nshake = re.sub(r'Scene \\w+', '', shake)\nshake = text_cleaner(shake[:int(len(shake)/60)])\nprint(shake[:100])",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Actus Primus. Scoena Prima. Enter Flauius, Murellus, and certaine Commoners ouer the Stage. Flauius.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Parse our cleaned data.\nshake_doc = nlp(shake)",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Group into sentences.\nshake_sents = [[sent,\"Shakespeare\"] for sent in shake_doc.sents]\n\n# Combine shakespear and Alice sentences\nsentences = pd.DataFrame(shake_sents + alice_sents)\n\n# Set up the bags.\nalicewords = bag_of_words(alice_doc)\npersuasionwords = bag_of_words(persuasion_doc)\nshakewords = bag_of_words(shake_doc)\n\n\n# Combine bags to create a set of unique words.\ncommon_words = set(alicewords + shakewords)\n\n\n# Get BoW Features\nword_counts = bow_features(sentences, common_words)",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Processing row 0\nProcessing row 50\nProcessing row 100\nProcessing row 150\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create our data frame with features. This can take a while to run.\nword_counts = bow_features(sentences, common_words)\nword_counts.head()",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Processing row 0\nProcessing row 50\nProcessing row 100\nProcessing row 150\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 68,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>help</th>\n      <th>odd</th>\n      <th>truly</th>\n      <th>thousand</th>\n      <th>come</th>\n      <th>end</th>\n      <th>stone</th>\n      <th>Christmas</th>\n      <th>Rome</th>\n      <th>labouring</th>\n      <th>...</th>\n      <th>bond</th>\n      <th>high</th>\n      <th>Rule</th>\n      <th>lock</th>\n      <th>bed</th>\n      <th>hardly</th>\n      <th>Speake</th>\n      <th>wise</th>\n      <th>text_sentence</th>\n      <th>text_source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>(Actus, Primus, .)</td>\n      <td>Shakespeare</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>(Scoena, Prima, .)</td>\n      <td>Shakespeare</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>(Enter, Flauius, ,, Murellus, ,, and, certaine...</td>\n      <td>Shakespeare</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>(Flauius, .)</td>\n      <td>Shakespeare</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>(Hence, :, home, you, idle, Creatures, ,, get,...</td>\n      <td>Shakespeare</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 560 columns</p>\n</div>",
            "text/plain": "  help odd truly thousand come end stone Christmas Rome labouring  \\\n0    0   0     0        0    0   0     0         0    0         0   \n1    0   0     0        0    0   0     0         0    0         0   \n2    0   0     0        0    0   0     0         0    0         0   \n3    0   0     0        0    0   0     0         0    0         0   \n4    0   0     0        0    0   0     0         0    0         0   \n\n      ...      bond high Rule lock bed hardly Speake wise  \\\n0     ...         0    0    0    0   0      0      0    0   \n1     ...         0    0    0    0   0      0      0    0   \n2     ...         0    0    0    0   0      0      0    0   \n3     ...         0    0    0    0   0      0      0    0   \n4     ...         0    0    0    0   0      0      0    0   \n\n                                       text_sentence  text_source  \n0                                 (Actus, Primus, .)  Shakespeare  \n1                                 (Scoena, Prima, .)  Shakespeare  \n2  (Enter, Flauius, ,, Murellus, ,, and, certaine...  Shakespeare  \n3                                       (Flauius, .)  Shakespeare  \n4  (Hence, :, home, you, idle, Creatures, ,, get,...  Shakespeare  \n\n[5 rows x 560 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "y = word_counts['text_source']\nx = word_counts.drop(['text_sentence', 'text_source'], 1)\n\n# SVM model using pca \nfrom sklearn.svm import SVC\nsvm = SVC(C=10, gamma=.01)\n\nsvm.fit(x, y)\n\n# Use train_test_split to create the necessary training and test groups\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=2000)\nprint('With 20% Holdout: ' + str(svm.fit(X_train, y_train).score(X_test, y_test)))\nprint('Testing on Sample: ' + str(svm.fit(x, y).score(x, y)))\n\n# Cross validating using 10 folds  \nfrom sklearn.model_selection import cross_val_score\nprint(cross_val_score(svm, x, y, cv=5))\n\nfrom sklearn.metrics import classification_report\nprint('SVM report :')\nprint(classification_report(y_test, svm.predict(X_test)))\n\n\n\nsvm_Emma_predicted = svm.predict(x)\npd.crosstab(y, svm_Emma_predicted)",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": "With 20% Holdout: 0.8333333333333334\nTesting on Sample: 0.9106145251396648\n[0.72222222 0.88888889 0.86111111 0.83333333 0.82857143]\nSVM report :\n              precision    recall  f1-score   support\n\n     Carroll       0.92      1.00      0.96        56\n Shakespeare       1.00      0.69      0.81        16\n\n   micro avg       0.93      0.93      0.93        72\n   macro avg       0.96      0.84      0.89        72\nweighted avg       0.94      0.93      0.93        72\n\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 80,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>col_0</th>\n      <th>Carroll</th>\n      <th>Shakespeare</th>\n    </tr>\n    <tr>\n      <th>text_source</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Carroll</th>\n      <td>129</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Shakespeare</th>\n      <td>16</td>\n      <td>34</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "col_0        Carroll  Shakespeare\ntext_source                      \nCarroll          129            0\nShakespeare       16           34"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Model 2"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Group into sentences.\nshake_sents = [[sent,\"Shakespeare\"] for sent in shake_doc.sents]\n\n# Combine shakespear and Alice sentences\nsentences = pd.DataFrame(shake_sents + persuasion_sents)\n\n# Set up the bags.\nalicewords = bag_of_words(alice_doc)\npersuasionwords = bag_of_words(persuasion_doc)\nshakewords = bag_of_words(shake_doc)\n\n\n# Combine bags to create a set of unique words.\ncommon_words = set(persuasionwords + shakewords)\n\n\n# Get BoW Features\nword_counts = bow_features(sentences, common_words)",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Processing row 0\nProcessing row 50\nProcessing row 100\nProcessing row 150\nProcessing row 200\nProcessing row 250\nProcessing row 300\nProcessing row 350\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y = word_counts['text_source']\nx = word_counts.drop(['text_sentence', 'text_source'], 1)\n\n# SVM model using pca \nfrom sklearn.svm import SVC\nsvm = SVC(C=10, gamma=.01)\n\nsvm.fit(x, y)\n\n# Use train_test_split to create the necessary training and test groups\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=2000)\nprint('With 20% Holdout: ' + str(svm.fit(X_train, y_train).score(X_test, y_test)))\nprint('Testing on Sample: ' + str(svm.fit(x, y).score(x, y)))\n\n# Cross validating using 10 folds  \nfrom sklearn.model_selection import cross_val_score\nprint(cross_val_score(svm, x, y, cv=5))\n\nfrom sklearn.metrics import classification_report\nprint('SVM report :')\nprint(classification_report(y_test, svm.predict(X_test)))\n\n\n\nsvm_persuasion_predicted = svm.predict(x)\npd.crosstab(y, svm_persuasion_predicted)",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": "With 20% Holdout: 0.8835616438356164\nTesting on Sample: 0.9315068493150684\n[0.8630137  0.90410959 0.89041096 0.89041096 0.89041096]\nSVM report :\n              precision    recall  f1-score   support\n\n      Austen       0.92      1.00      0.96       127\n Shakespeare       1.00      0.42      0.59        19\n\n   micro avg       0.92      0.92      0.92       146\n   macro avg       0.96      0.71      0.78       146\nweighted avg       0.93      0.92      0.91       146\n\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 82,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>col_0</th>\n      <th>Austen</th>\n      <th>Shakespeare</th>\n    </tr>\n    <tr>\n      <th>text_source</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Austen</th>\n      <td>315</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Shakespeare</th>\n      <td>25</td>\n      <td>25</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "col_0        Austen  Shakespeare\ntext_source                     \nAusten          315            0\nShakespeare      25           25"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "49px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}